EEVEE and GATE: Finding the right benchmarks and how to run them seamlessly

Anonymous Author(s)
Affiliation
Address
email

Abstract

Model evaluation is a cornerstone of machine learning, guiding model design and progress measurement. Designing generalizable evaluation processes remains a challenge, however, partly due to the vast number of possible domain, task and modality combinations and lack of knowledge of how informative they are. In this paper, we propose EEVEE (Efficient Evaluation process Evolution Engine)\(^1\), a method that frames evaluation process design as a learning problem. By analyzing a large number of evaluation metrics from diverse benchmarks and models, EEVEE identifies a smaller subset of tasks with high predictive power over the full set of evaluation metrics, reducing evaluation time. To find the optimal subset maximizing signal while minimizing GPU hours, EEVEE evaluates pre-trained models of various architectures, pretraining schemes, and modalities on diverse downstream tasks and datasets including image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression. Our results identify three subsets of benchmarks, with 8, 15 and 21 tasks, providing high quality signal for model generalization. Key benchmarks selected include iWildCam, CLEVR-Math, ACDC, WinoGround, CIFAR100, Fungi, and ADE20K. We structure the subsets into three tiers for 12, 24, and 36 GPU-hour budgets and package them into a unified, efficient, and user-friendly Python framework that we built with the researcher in mind – which we refer to as the GATE engine. Our experiments reveal ConvNextV2, SigLIP and CLIP as top-performing model encoders, with EfficientNetV2 and ResNext50 excelling in medical tasks and challenging image classification, in particular in Happy Whale Individual classification, ConvNet based models seem to outperform transformer models by a factor of 2.5x, which is surprising. The top performing encoder being ConvNextV2 followed by CLIP seems to agree with other recent large scale evaluations. We also demonstrate the framework’s versatility in fine-tuning models from text and audio modalities, paving the way for future cross-modal evaluations.

1 Introduction

Increasing Complexities of Benchmarking: As we create benchmarks for expanding model capability evaluation, the growing number and complexity of these benchmarks inadvertently complicates evaluation, requiring more resources like engineering, computation, and research time. Consequently, prioritizing which benchmarks to use becomes challenging. The high costs and longer wait times of newer, complex benchmarks often deter their adoption, leading researchers to rely on older, simpler benchmarks. This risks missing valuable insights from innovative ideas that may underperform on

\(^1\)Pronounced as /ˈiːviː/ EE-vee

Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute. simpler benchmarks but have broader applicability, while promoting incremental improvements that
overfit to simpler benchmarks but underperform in comprehensive evaluations.

To illustrate the mounting increase in available benchmarks, we can look at the historical benchmarks
in deep learning. Few benchmarks have had as much impact as ImageNet [29], which remains a
rich resource for model training and evaluation, particularly in visuo-linguistic models. As key
capabilities for deep neural networks were discovered, more benchmarks were generated to measure
and stimulate progress in those areas. In natural language processing, the GLUE benchmark [65],
SQuAD [45], and CoNLL-2003 [48] have been instrumental. In audio processing, LibriSpeech [39],
TIMIT [15], and VCTK [68] are widely used. For machine translation, WMT [3], IWSLT [22], and
Europarl [25] have driven advancements. Relational reasoning has been advanced by benchmarks
such as CLEVR [23], bAbI [66], and RAVEN [71]. In segmentation, PASCAL VOC [14], Cityscapes
[8], and COCO [33] remain crucial. Large language models are often evaluated using benchmarks
like SuperGLUE [64], LAMBADA [40], and MMLU [19]. Vision-language models are typically
evaluated using benchmarks such as VQA [1], Visual7W [76], and Flickr30k [42].

As a result, a researcher has to choose from all these options, and even more, and then find a
way to unify and experiment with their models across all of them. The lack of unification, and
the lack of guarantees for their generalization signal, quickly becomes a kind of “evaluation hell”,
where researchers waste a lot of time just doing redundant things like fixing the same bugs to
download datasets, preprocess them etc, while at the same time not having any real signal as to which
benchmarks are more informative, other than just knowing what has been used the most – which is
usually a function of popularity, and not real informativeness. To elaborate, the adoption of complex
evaluation processes that could enhance research efficiency and impact is often hindered by the
engineering effort required to evaluate machine learning models. Researchers must create involved
pipelines across multiple datasets demanding high data engineering efforts, develop task-specific
adapters, and derive nuanced training recipes, which is time-consuming. As a result, researchers
often revert to simpler evaluation strategies instead of comprehensive assessments.

A good benchmark should alleviate these burdens by automating dataset handling, integrating task
adapters, optimizers, schedulers, and logging mechanisms seamlessly. It should provide broad and
meaningful signals with minimal GPU time, accommodating various computational budgets, ensuring
inclusivity. Furthermore, an increasingly important factor for a robust modern benchmark engine
is its support for multi-modal learning and early fusion techniques. AI systems must seamlessly
integrate and reason across multiple modalities, such as text, images, audio, and more. Multi-modal
learning enhances self-supervised learning opportunities and provides inherent supervision through
natural alignments, like audio-visual synchronization in videos. Early fusion, where data from
different modalities is combined at the initial stages of processing, allows models to leverage shared
representations, improving generalization and reasoning capabilities across varied tasks and domains.
These key desiderata are what motivates the production of this work.

With the desiderata in mind, we next introduce EEVEE, a methodology developed for building
high-signal low-cost evaluation routines, and GATE, the resulting benchmark that is designed to
be extensible, readable, flexible, modular and robust, supported by a new efficient, easy to use
framework.

**EEVEE, Learning Optimal Benchmarks:** The ability to find which benchmarks offer the most
signal with respect to a given goal, such that we can optimize our compute time, research iteration
speed, and engineering time is increasingly crucial. In this work, rather than just manually designing
a new set of benchmarks, we propose a methodology, called **EEVEE (Empirical Evaluation process
Evolution Engine)** that frames evaluation design as a learning problem and then leverages machine
learning to automate the discovery and refinement of evaluation processes.

More specifically, EEVEE operates by taking in a large set of performance metrics from diverse
models applied across various benchmarks and identifies a smaller subset of benchmarks with high
predictive power over the entire set. EEVEE achieves this through two main components: (a) an
evolutionary algorithm to optimize the selection of benchmark combinations based on a computed
score, and (b) a meta-model trained to predict a model’s performance on the full set of benchmarks
using performance metrics from a chosen subset. We parameterize the meta-model as as a small
neural network. The meta-model receives input performance metrics from a subset of benchmarks and predicts performance on the full set of performance metrics. Through careful $k$-fold cross-validation and leveraging a diverse set of models and benchmarks, EEVEE iteratively evolves benchmark combinations that offer high information content with respect to the entire spectrum of benchmarks, ensuring robust, efficient and comprehensive evaluation that can be targeted to computational budgets ranging from more “GPU Poor” users to high-budget organizations.

Taking the desiderata explained above and the resulting understanding of what a good evaluation engine should look like, we demonstrate the effectiveness of EEVEE by tasking it with the discovery of benchmark combinations that offer good **signal-to-GPU-time** ratio, for the evaluation of **model encoders** – also referred to as backbones, on their ability to adapt to new tasks, domains, and modalities. For this purpose, we choose a pool of 20 models, varying in their pretraining schemes (e.g. CLIP, DINO, ImageNet Classification), architectures (e.g. ResNets, ViTs, ConvNext) and even their source modalities (e.g. Whisper, BERT), which we adapt on 31 benchmarks ranging from image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression, using robust fine tuning recipes, and training for 10K iterations, ensuring that the signal we get is about models that are adaptable, generalizable and efficient in their adaptation.

By applying 20 models on 31 benchmarks and employing EEVEE on their resulting metrics, we identify three subsets of benchmarks, each targeted to a specific computational budget range. Some of the key benchmarks that have been selected include iWildCam, CLEVR-Math, ACDC, WinoGround, mini-ImageNet, Fungi, ADE20K, and dtextures. We refer to the discovered subsets as **Tiers**, and assign to them identifiers for their sizes, specifically, **small** (n=8, 12 GPU hours), **base** (n=15, 24 GPU hours) and **big** (n=31, 36 GPU hours). We package these tiers into our comprehensive benchmarking suite and software framework (called **GATE**) designed for domain, task and modality transferability evaluation, which facilitates the transfer of neural network encoders to different modalities, domains, and tasks. GATE’s architecture caters to the research community, enabling straightforward replacement of these transferable encoders with minimal effort. With these innovations, GATE seeks to evolve the landscape of model encoder evaluation, championing a deeper understanding of transfer learning and model adaptability.

**Contributions:** 1. We introduce **EEVEE**, a machine learning approach for selecting subsets of benchmarks optimized to offer maximal predictive power over a larger benchmark set. 2. We conduct a comprehensive investigation of diverse benchmarks within the space of image, image+text and video modalities, pinpointing those with the highest predictive value for a model’s performance in downstream tasks. We apply EEVEE to model encoder evaluation by training 20 models on 31 benchmarks, identifying subsets of 8, 15 and 21 benchmarks that offer high signal-to-GPU-hour ratios. 3. We pack the EEVEE-discovered subsets (of 8, 15 and 21 benchmarks out of 31 benchmarks) into targeted benchmark packs, referred to as tiers, designed for specific compute budgets (of 12, 24 and 36 GPU hours) and project phases, and establish standard experimental settings for these tiers. We call these collectively as the **GATE** Benchmarks. 4. We develop the **GATE** engine, a unified benchmark suite and software framework that automates dataset downloading, preprocessing, and pipelining for fine tuning and evaluation. GATE facilitates the incorporation of new model encoders, adapts input modalities, fine-tunes with robust recipes, and logs critical information such as training and evaluation metrics, power, energy, computational usage, task visualizations, and model gradients per layer. 5. Through our extensive investigation, we identify foundation models demonstrating superior transferability across diverse tasks. 6. We advocate for the inclusion of modality-shifting transfer experiments in the standard evaluation process for ML researchers, supported by our experimental results on the performance of existing foundation models in these benchmarks.

## 2 Related Work

**On the Diversity of Benchmarks:** There is a vast array of benchmark suites in machine learning. To the best of our knowledge, the benchmark suites relating strongly to GATE are ImageNet [9], VTAB [70], VLMBench [73] and WILDS [26]. ImageNet has been of tremendous importance and interest to the transfer learning community. Nevertheless, there has been skepticism about overfitting to such datasets resulting from implicitly qualifying models using the test set performance over the years [46,6] or the test set not being challenging enough to gauge model generalization power [47]. Although ImageNet pre-training helps transfer performance to the many-shot classification setting [13], it provides minimal to no gains on more challenging datasets such as fine-grained classification [27]. Similarly, with a larger distribution shift, ImageNet pre-trained models was found to offer limited benefits for medical imaging tasks due to large distribution shifts induced by fundamental differences in data sizes, features, and task specifications; that is, lightweight models perform comparably to standard architectures [44]. To make matters worse, ImageNet performance is less correlated with and less predictive of downstream performance on diverse tasks beyond classification such as object detection, few-shot classification, and segmentation [13]. On top of it all, when ImageNet is extended with a perturbed temporal dimension, models performance significantly worsen [52].

**On the Usability of Benchmarks:** Beyond ImageNet, VTAB introduced a benchmark with a wider diversity of tasks and domains [70]. Nevertheless, it does not offer task and domain shifts offered in GATE, such as medical segmentation and video classification and regression that are known to be ill-measured and gauged by ImageNet alone [44, 52]. That said, VTAB offers satellite imaging and 3D tasks which GATE does not. Nevertheless, GATE as a software framework was optimized to minimise usage friction, to take no more than 12 GPU hours on our smallest tier, and, to only require approximately 1 hour of adding the new encoder and wrapping it into GATE wrappers for GATE to be able to go away and take care of everything, including dataset downloading, task adapter integration and full train/val and test cycles with logging of various key metrics. VTAB, in our experience, requires a lot more manual work in getting the datasets, and integrating new models to be adapted. Similarly, VLMBench [73] and WILDS [26] offer more diverse datasets beyond previous work but neither offer a tiered approach that enables iterative development of models during pre-training, nor produce extensible and flexible benchmarks that can be easily glued into researchers experimentation code without friction.

**On the Systematic Selection of Benchmarks:** Previous work investigated the properties inherit in multi-task benchmarks that trade-off diversity and sensitivity where the latter is how robust a benchmark ranking is to the inclusion of irrelevant models or minute changes in the tasks themselves [72]. It was found that multi-task benchmark are unstable to irrelevant changes in tasks design. Nevertheless, this is related to how the benchmark ranks models; whether it compares how model often ranks higher than another in cardinal benchmarks or if the performance across tasks is averaged to produce a single rank in cardinal ones. Meanwhile, our benchmark produces fine-grained information to model performances across diverse tasks rather than producing specific ranking which is delegated to the user analysis. Another complementary thread of work investigates dynamic benchmarks where model training and data collection is interleaved to continually challenge model knowledge [53]. To the best of our knowledge, this is the first work that studies the selection of multi-task, multi-domain benchmarks that satisfy limited compute budgets while maximizing research signal.

In summary, Table 1 shows the desiderata that we believe a good evaluation suite and framework should have such that they can both offer the community useful signal, and also balance that with being practical so that people can adopt it.

### 3 EEVEE Methodology

EEVEE is our proposed method for automating the selection of Pareto-optimal benchmark subsets. By analyzing benchmark performance metrics, EEVEE identifies a small, highly informative subset that maximizes information relative to the entire benchmark pool. This ensures that, as machine learning benchmark breadth and depth increases, we will always be able to identify and select few that offer high information about the whole. We strike a balance between providing rich evaluation signals and maintaining simplicity, reducing computational costs and human efforts required for adopting new benchmarks. EEVEE enables the production of a tiered evaluation engine accommodating various computational budgets, fostering an inclusive and accessible research environment, and improving the quality of insights derived from machine learning research while addressing reluctance. towards resource-intensive evaluation processes. This balance between efficiency, simplicity, and signal richness presents EEVE’s value proposition for advancing machine learning research.

**Working Principle of EEVEE:** EEVEE works by building a *meta-model* over the performance metrics of models sufficient both in number and diversity, on the full benchmark pool from which we want to choose our subset. With the term *benchmark* in this paper we refer to a dataset + task pairs.

Formally, given a large benchmark pool $B = \{b_0, b_1, \ldots, b_K\}$, where $B$ is the full set of benchmarks, and $b_i$ are individual benchmarks therein, we have a sufficiently large and diverse pool of model performance metrics $M = \{m^0_i, m^1_i, \ldots, m^K_i\}$. Here, $m^j_i$ is the performance metric of model $j$ on benchmark $b_i$. We aim to discover a subset of $B$ of size $k$. This means $k$ total benchmarks make up the subset. If we build a meta-model $g(M_{selected}, \theta)$ to predict all of $M$ given only the selected subset $M_{selected}$, it should minimize the following loss:

$$L_{EEVEE} = \text{MSE}(M, g(M_{selected}, \theta))$$  \hspace{1cm} (1)

In this equation, MSE is the mean squared error. $M$ represents the full set of performance metrics of all our models on the full benchmark pool $B$. The term $g(M_{selected}, \theta)$ represents the predictions of the meta-model $g$ with parameters $\theta$ when it is given the performance metrics of all models from the selected subset of benchmarks $B_{selected}$, referred to as $M_{selected}$.

However, our main focus lies in the selected combination of performance metrics $M_{selected}$ that can generalize well on previously unseen models. To that end, we must split $M$ into train, validation and test sets, each consisting of performance metrics acquired from different models (e.g. train $\rightarrow$ ResNet50, ViT-Base, CLIP, and val $\rightarrow$ ResNext50, DINO, DeIT), and explicitly optimize the inner loop test loss rather than the training loss, while we use the validation loss to select the best meta-model for test. Hence the loss we wish to minimize is:

$$L_{test}^{EEVEE} = \text{MSE}(M^{test}, g(M^{test}_{selected}, \theta))$$  \hspace{1cm} (2)

We need a non-differentiable method for choosing the $k$ benchmarks in $M_{selected}$, since brute force becomes intractable very quickly, so we employ evolutionary methods to learn the $k$ selected benchmarks.

This results in a bi-level optimization, with an evolutionary method on the outer loop $e(B_{selected})$, where $e$ is the evolutionary method, and $B_{selected}$ are the benchmarks being selected – or indeed, the genes being optimized, and a small meta-model parameterized as a neural network $g(\theta)$ that receives a train/val split from $B_{selected}$ and trains itself to do the task described in Equation 1 after which process it is scored using the val set using the loss in Equation 2. Then, once a given candidate of benchmarks $B_{selected}$ is scored, in this way, the outer loop performs a tournament selection where only the top 50 candidates are preserved and mutated by removing one benchmark at random, and adding another at random. Each winning candidate mutates into 10 children, and the parent is also preserved in the gene pool, producing a gene pool with 550 candidates for every cycle. At initialization, we sample 1000 random combinations. We have found that 1000 is a good starting population that is both tractable to score and facilitates the necessary diversity that enables limited variation in results across several runs, showcasing convergent behaviour. diversity that our results across runs have little variation from one another, pointing to a convergent behaviour. We include full pseudocode showcasing all the details related to how we performed EEVEE for our experiments in Algorithm 1, 2 and 3 in Figure 1.

**Applying EEVEE on Model Encoder Generalization**

**Why Model Encoder Evaluation?** A common practice across machine learning applications involves augmenting general model encoders with task-oriented heads. The adaption of this paradigm can be attributed to the computational efficiency associated with training model encoders, over more expensive setups. Much of computer vision, as well as vision to text search and retrieval happen using model encoders. Similarly, various applications requiring translation from one domain/modality/task to another require an encoder of some sort. Even the “decoder-only” LLM models that have demonstrated incredible capabilities in the last few years, internally can be seen as a series of representation encoders, a series of refinement before they reach the decoding stage. Multi-modal early fusion is another topic closely related with model encoders â as research in early fusion can be done most efficiently when trying to learn data encoders rather than a full encoder-decoder, or decoder-only models. World model research in multi-modal dimensions can also take place most efficiently within a model-encoder context. Recent works like I/VJEPA [2] for example have paved the way for self-supervised learning which functions using model encoders, and has been demonstrated to be more efficient and more generalizable than full pixel decoding variants.\n\nFurthermore, model encoder evaluation has been quite diffused in the past few years, with new benchmarks being produced in every facet of the machine learning field. Nonetheless, most of those lacked in some key quality: they were either simply too complex to use efficiently, requiring too much compute, or, more often than the others, missing a unifying software framework that can easily, in a user-conscious way, and a principled stance towards high readability, maintainability and hackability.\n\n**The goal of focusing on Model Encoder Evaluation:** By applying EEVEE to search for a pareto-optimal set of benchmarks, and packaging it up in a unified framework that is built for the researcher in mind from the ground up, one which offers out of the box automated downloading, pipeline building, task adapters, and a very mature training and eval loop. Within this framework, we facilitate, all relevant logging information, including key training and eval metrics, rich gradient information, power and computational information, as well as visualizations where relevant. Finally, we support easy switching of model encoders, no matter what source modality they come from â our framework dubbed GATE is a one stop shop for ones model representation research needs, both during research, debugging, as well as at the evaluation phase.\n\nGATE comes in three tiers small, base and big-GATE. Each having 8, 15 and 21 benchmarks within it, and targetted towards 12/24 and 36 GPU hours on a A100 40GB. We hope that by making it very easy for the end user and offering such rich signal for machine learning research, many researchers will choose to use GATE, to enhance their research signal, whilst keeping the compute budgets relatively feasible.\n\n**Preparations: Choosing Models, Benchmarks and Adaptation Processes:** EEVEE will yield better results if the space of models, benchmarks and adaptation processes we use is diverse, but also thorough in numbers. **A. Adaptation Process** We wanted GATE to cover multiple domains, tasks and modalities when shifting from the source to the target setting. For that reason we decided that if a model encoder has an input layer that does not fit the target modality, we simply remove that input layer and replace it with a relevant ViT-like patchification [12] followed by a linear combination for each patch. For tasks where we have text, we would tokenize the text using BPE [51], and for tasks where we have video we would use the model encoder on each image, to acquire an image-level vector representation, and then follow that up with a simple 4 layer transformer that receives a sequence of image-vector tokens, to produce a video-level embedding, on top of which we apply the task-specific head at hand. The task-adapters we used leaned on established methods, and where possible we just used a transformer head, which includes segmentation, relational reasoning and video classification, with everything just using a linear head, full details available at [14]. After these\n\n---\n\n**Algorithm 1 Scoring**\n\n**Require:** Performance metrics $M$, Input metrics $M_{\\text{input}}$.\n- Epochs $E = 20$, Hidden dimension $d_{\\text{hidden}} = 100$.\n- Learning rate $\alpha = 0.01$, Weight decay $\lambda = 0.01$, Optimizer type $\\omega = \\text{ modifications, described in Figure 2, we use a fine tuning scheme – this decision was informed by preliminary experiments on both full fine tuning and linear probe with a frozen backbone, in which we found that there was a clear superiority of fine tuning over linear probing for the benchmarks we chose in our pool. Full details of these preliminary experiments can be found in Appendix 8.1. In our preliminary experiments we were able to identify three recipes, one for ConvNet-style architectures, one for ViT-style architectures and one for Hybrid architectures such as ConvNext and ResNext that worked well for all tasks, details in 8.1.

B. Model Pool We wanted the space of models used to cover many important pretraining schemes, architectures, and source modalities. The details of these choices are provided next: 1. Pretraining Task and Dataset Variation: With a consistent architecture, models were subjected to various pretraining tasks and datasets. Model instances representing this category include CLIPViT [43], ConvNextV2 [35], Siglip, FlexViT [7], LaionViT, ImageNet1K ViT [11] with Random Augment, SAM-ViT, DiNoViT, EfficientFormerV2 [32] and DeiT3 [59]. Further to these, we include models initialized from scratch, specifically, ViT, ResNet50 [18], FlexViT, EfficientNetV2 [57], and then fine-tuned on the GATE tasks. 2. Architectural Variation: We explored models having the same pretraining dataset (ImageNet), but differing in their architecture. This group encompassed a mix of standard CNN models such as EffNetV2, ResNet50, ResNext50 [67], ConvNextV2_Base [35] and transformer-based models like EfficientFormer [32] and FlexViT [7]. 3. Modality and Dataset Variation: This axis comprised models trained on modalities other than vision such as Whisper, coming from an audio to text task and Bert [10], Bart [31] and Mpnet [55] coming from various text-based tasks. These models had their original input processing systems replaced by a Vision Transformer style embedding and were subsequently fine-tuned on the GATE tasks. A more comprehensive account of these models, including their selection rationale and unique characteristics, is provided in the Appendix Section 13.

C. Benchmark Pool The benchmark pool, detailed in the Appendix, includes Image Classification (ImageNet1k [9], CIFAR100 [28], Places365 [74], Food101 [36], HappyWhale [17]), Few Shot Image Classification (Aircraft [37], Fungi [50], MiniImageNet [62], CUB200 [63], Describable Features [69]), Zero Shot Text-Image Classification (Flickr30K [41], New Yorker Caption Context [20], Winoground [58]), Visual Relational Reasoning (CLEVR [23], CLEVRMath [34]), Image Semantic Segmentation (ADE20K [75], COCO10K [33], COCO164K [33], NYU-Depth-v2 [54], PascalContext [38], Cityscapes [8]), Medical Image Classification (Chexpert [21], Diabetic Retinopathy [16], HAM10000 [60]), Medical Segmentation (ACDC [5]), Video Classification (HMDB51 [30], UCF-101 [56], Kinetics400 [24]) and Video Regression (iWildcam [4]).

Producing Diverse Model Performance Metrics: We apply our adaptation process on each and every model chosen, on every benchmark in the benchmark pool. To acquire test results we ensemble by averaging logits of the top 1, 3 and 5 validation models to produce three separate ensemble results.

D. Experimental Approach We wanted our research environment to reflect the end user, so we can properly understand their needs, and to offer a pragmatic experimental setup of in-the-wild researchers with little time to hyperparameter optimize, and which have to make decisions on small amounts of preliminary experiments – someone choosing a model encoder off the shelf and adapting it to downstream setting. For that reason, we kept any hyperparameter tuning, or human attention when it came to specific models to a minimum. Instead, we relied on existing good recipes, and did some preliminary experiments as explained in detail in 8.1. Briefly, we discovered specific adjustments for each architecture type: for Convolutional Architectures, we used AdamW with a learning rate of 1e-3, and 6e-4 for segmentation tasks; for Vision Transformer Architectures, AdamW with a learning rate of 1e-5; and for Convolutional + Transformer Hybrid Architectures, AdamW with a learning rate of 2e-5. A plateau learning rate scheduler was configured with parameters like mode "min", factor 0.5, patience 1000, and threshold 1e-4, allowing models to effectively choose their own schedules based on their learning progress. This adaptive scheduling facilitated “good enough” learning rates and enhanced performance across different architectures.

4 Results

Single Benchmark Predictiveness: As demonstrated in Figure 3 using EEVEE we quantified the predictive power of each benchmark on its own, when not in a combination with others. We have found that ADE20K, Flickr30K, and the New York Caption Competition lead in their predictive power, with few-shot tasks, and relational reasoning, being very close to the best in predictive power. ImageNet1K sits squarely in the middle of the competition. Furthermore, some of the most “novel” Figure 3: The EEVEE MSE Loss (k=1) shows "predictiveness over the whole," with lower values being better. Benchmarks like iWildcam, HappyWhale, and WinoGround test unique capabilities and may not predict all tasks, yet EEVEE often includes at least two of these in its top combinations along with a “natural-image representative” such as CIFAR100, ADE20K or Flickr30K.

(a) Small-GATE (k=8, 12 GPU hour) tier  
(b) Small-GATE (k=15, 24 GPU hour) tier  
(c) Small-GATE (k=21, 24 GPU hour) tier

Figure 4: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different GATE tiers.

benchmarks like iwildcam, happy whale, ACDC, NYU and Winoground are the least predictive tasks, Winoground being magnitudes less predictive. We argue that this is mainly due to the tasks being “harder”, and our models being less designed for those. The results in WinoGround were bearly better than chance for example. However, when once we move to combinations of benchmarks, these ‘less’ predictive benchmarks become key contributors to better predictive power, as they represent edge cases, as can be seen in Figures 6g 7c 7i where these have the highest importance when removed from a given set.

Predictiveness of Discovered Combinations In Figure 5 we can see how the top-50 performing candidate combinations perform as we vary the number of benchmarks per combination from 1 to 26. We can see that there is a point of diminishing returns around the $k = 8$ point, after which there appears to be some “overfitting” occurring. We verified that the overfitting was a result of having a small sample number of 20 models, to train, val and test our meta-models with, as well as the 2-layer MLP we used to model Few-to-All metric predictions. We tried our level best to find the best architecture and regularization schemes for our meta-model, and this was the best we could do given available compute and (human) time. We chose 8, 15, and 21 as the combination-threshold to make our packs out of as they satisfied the computational budgets we set for ourselves, and they have very diverse and predictive tasks, as can be seen in Figures 6g 7c 7i. For full details on all the discovered top-k combinations please look at Appendix Section 16.1.

Best Models based on GATE: As can be seen in Table 2 or the Appendix extended Table 3, the best overall models are ConvNextV2, SigLIP and CLIP in that order, with SigLIP and CLIP often exchanging ranks between themselves. However, it is worth noting that EfficientNetV2 demonstrated exceptional performance/compute across all tasks, and even outperformed all models in many medical tasks. Finally, ConvNet based models, and particularly ResNext50 seem to have done exceptionally well in the edge-case scenarios of ACDC, Happy Whale Individual identification, and Table 2: Summary of experiments: Black/Bold best model, Green second best, Blue third best, and red the worst performing model. Models prefixed with 's' refer to 'from scratch' trained models, rather than pretrained. For the full table look at Appendix Table 3.

**Limitations:** We empirically evaluated EEVEE on a relatively large pool of models and benchmarks, however, with more models, and benchmarks it could yield much more general results. Especially with benchmarks targeting the text and audio modalities, as well as potentially offline RL.

5 Conclusion

In this paper, we propose EEVEE, an evolutionary-method-based search algorithm that can discover out of a large collection of benchmarks, the ones that can offer the most predictive value on the original collection, for a given set of models. We apply EEVEE on the task of model-encoder evaluation in the context of images, image-text, videos, and medical domains. As a result, we obtain the GATE Benchmark, which consists of 3 tiers, each targeted to a particular GPU budget, from 12, 24 and 36 GPU hours, per model evaluation. We then introduce the GATE engine, which takes these benchmarks, and offers a researcher-designed environment in which one can easily port their own model encoder, and run the full GATE tiers, and automatically produce a variety of performance, energy/power, hardware utilization metrics and task visualizations. We evaluated 20 representative models ranging from image, image-text, text and audio pretrained models, on the GATE tiers, and we discovered that ConvNextV2 and SigLIP seem to lead the pack overall, with EfficientNetV2 being an exceptional, efficient alternative for the medical domain and for unique scenario tasks, such as Happy Whale, ACDC and WinoGround. Finally, ConvNet based models, and ResNext50 in particular, seem to have a lot more learning efficiency, as they are the best adapted models on very novel domains, such as Happy Whale individual prediction challenge, ACDC and medical tasks. References

[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2425–2433, 2015.

[2] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video, 2024.

[3] Loic Barrault, Ondrej Bojar, Marta R Costa-jussa, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, et al. Findings of the 2019 conference on machine translation (wmt19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, 2019.

[4] Sara Beery, Grant Van Horn, and Pietro Perona. The iwildcam 2018 challenge dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 54–60, 2018.

[5] Olivier Bernard, Alain Lalande, Caio Zotti, Florence Cervenansky, Xin Yang, Pheng-Ann Heng, Ismail Cetin, Karim Lekadir, Oscar Camara, Miguel A Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: Is the problem solved? IEEE Transactions on Medical Imaging, 37(11):2514–2525, 2018.

[6] Lucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with imagenet?, 2020.

[7] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim M. Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14496–14506, 2022.

[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Tobias Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3213–3223, 2016.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Li Kai, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.

[13] Linus Ericsson, Henry Gouk, and Timothy M. Hospedales. How well do self-supervised models transfer? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5414–5423, June 2021.

[14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303–338, 2010. [15] John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G Fiscus, David S Pallett, and Nancy L Dahlgren. Timit acoustic-phonetic continuous speech corpus ldc93s1, 1993.

[16] Varun Gulshan, Lily Peng, Marc Coram, Michael C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Travis Madams, Jorge Cuadros, et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. *JAMA*, 316(22):2402–2410, 2016.

[17] Happywhale. Happywhale - whale and dolphin identification challenge, 2022.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 770–778. IEEE, 2016.

[19] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Mantas He, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. *arXiv preprint arXiv:2009.03300*, 2020.

[20] Jack Hessel. New yorker caption contest corpus, 2023.

[21] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Hagghoo, Robyn Ball, Katie Shpanskaya, et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. *Proceedings of the AAAI Conference on Artificial Intelligence*, 33:590–597, 2019.

[22] Niehues Jan et al. Iwslt 2017: Proceedings of the 14th international workshop on spoken language translation. 2017.

[23] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2901–2910, 2017.

[24] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. In *arXiv preprint arXiv:1705.06950*, 2017.

[25] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. *MT summit*, 5:79–86, 2005.

[26] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In *International Conference on Machine Learning*, 2021.

[27] Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better? In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2019.

[28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report UTML TR 2009, University of Toronto, Toronto, Ontario, Canada, 2009.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. *Advances in neural information processing systems*, 25:1097–1105, 2012.

[30] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: A large video database for human motion recognition. In *2011 International Conference on Computer Vision (ICCV)*, pages 2556–2563. IEEE, 2011.

[31] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. *arXiv preprint arXiv:1910.13461*, 2020. [32] Xiuyu Li, Yutong Yuan, Shu Chen, Martin Danelljan, Radu Timofte, and Luc Van Gool. Efficientformer: Vision transformers at mobilenet speed. *arXiv preprint arXiv:2206.01191*, 2022.

[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In *European Conference on Computer Vision*, pages 740–755, 2014.

[34] Adam Dahlgren Lindström and Savitha Sam Abraham. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. *ArXiv*, abs/2208.05358, 2022.

[35] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. *arXiv preprint arXiv:2201.03545*, 2022.

[36] Matthieu Guillaumin Lukas Bossard and Luc Van Gool. Food-101 – mining discriminative components with random forests. In *European Conference on Computer Vision*, 2014.

[37] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. In *2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 554–561. IEEE, 2013.

[38] Roozbeh Mottaghi, Xiaobai Chen, Xiaofeng Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 891–898, 2014.

[39] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus based on public domain audio books. In *2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pages 5206–5210, 2015.

[40] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1525–1534, 2016.

[41] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for grounded image descriptions. *arXiv preprint arXiv:1505.04870*, 2015.

[42] Bryan A Plummer, Liwei Wang, Christopher M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 2641–2649, 2015.

[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pages 8748–8763. PMLR, 2021.

[44] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding transfer learning for medical imaging. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 32. Curran Associates, Inc., 2019.

[45] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 2383–2392, 2016.

[46] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10?, 2018. [47] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In *Proceedings of the 36th International Conference on Machine Learning*, Proceedings of Machine Learning Research. PMLR, 2019.

[48] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In *Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003*, pages 142–147, 2003.

[49] Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In *Advances in Neural Information Processing Systems (NeurIPS)*, pages 4967–4976, 2017.

[50] Dirk Schroeder, Yin Cui, Yang Chai, Daniel Kristensen, Evangelos Kalogerakis, and Serge Belongie. The fgvcx fungi classification challenge. In *CVPR Workshops*, 2018.

[51] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1715–1725, 2016.

[52] Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do image classifiers generalize across time? In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 9661–9669, October 2021.

[53] Ali Shirali, Rediet Abebe, and Moritz Hardt. A theory of dynamic benchmarks, 2023.

[54] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In *Proceedings of the European Conference on Computer Vision (ECCV)*, pages 746–760, 2012.

[55] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. *arXiv preprint arXiv:2004.09297*, 2020.

[56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. In *arXiv preprint arXiv:1212.0402*, 2012.

[57] Mingxing Tan and Quoc V Le. Efficientnetv2: Smaller models and faster training. *arXiv preprint arXiv:2104.00298*, 2021.

[58] Tristan Thrush, Hongyu Jiang, Goutham Prasad, and Jacob Andreas. Winoground: Probing vision and language models for visio-linguistic compositionality. *arXiv preprint arXiv:2204.03162*, 2022.

[59] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, and Jakob Verbeek. Deit iii: Revenge of the vit. *arXiv preprint arXiv:2204.07118*, 2022.

[60] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. *Scientific Data*, 5:180161, 2018.

[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in Neural Information Processing Systems (NeurIPS)*, pages 5998–6008, 2017.

[62] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In *Advances in Neural Information Processing Systems (NeurIPS)*, pages 3630–3638, 2016.

[63] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. [64] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 3266–3280, 2019.

[65] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, 2018.

[66] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merrienboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.

[67] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1492–1500. IEEE, 2017.

[68] Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92). University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.

[69] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Fine-grained visual comparisons with local learning. In 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 192–199. IEEE, 2014.

[70] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. In International Conference on Learning Representations, 2020.

[71] Chi Zhang, Feng Gao, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5317–5327, 2019.

[72] Guanhua Zhang and Moritz Hardt. Inherent trade-offs between diversity and stability in multi-task benchmarks, 2024.

[73] Kaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, and Xin Eric Wang. VLMbench: A compositional benchmark for vision-and-language manipulation. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks, 2022.

[74] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 40, pages 1452–1464. IEEE, 2017.

[75] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5122–5130, 2017.

[76] Yuke Zhu, Olaf Groth, Michael S Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995–5004, 2016. 6 End-user Guidelines

For an end-user to use GATE, they need to:

1. Install the GATE framework python package, as described in the Github repo’s readme page.

2. Choose a path for implementing the new foundation model encoder they wish to evaluate. This is either cloning the full GATE repo and modifying existing components directly, or, importing the GATEncoder and GATEModel classes from GATE, and wrapping up their model within it. Doing so requires the researcher to implement a relevant forward function that can take in the modalities their model needs to process, as well as defining a configuration that tells GATE what modalities a model can receive and output features on, as well as any transforms needed for a batch to be ready for their model.

3. The user chooses a GATE tier to use (from smallGATE, baseGATE and bigGATE). Based on the configuration defined by the user in step 2.

4. GATE generates a list of commands, each representing an experiment that needs to be run, and can then run these commands on your local GPU box, parallelizing the tasks, one on each available GPU, or, can provide a list of commands or json file that one can use to run these commands on a GPU cluster, or other hardware.

5. GATE emits a wandb project, with metrics, visualizations and other measures, allowing easy tracking of experiments, and sharing thereof, as well as huggingface model weights for each model being trained – which is also used to achieve a stateless execution.

6. Once the experiments are completed, one can invoke the produce-analysis.py file within GATE to get tables and figures that analyse the data, similar to what appears in this paper. Those results can then be used to report results in a paper, or, be used to make decisions for production models.

This process ensures the GATE framework is aware of what a model’s supported modalities are, as well as how to produce modality-specific features, given the model. Once this is completed, the user, with a single line of code, can select a GATE tier, and launch all jobs needed to produce results for that tier. Importantly, GATE is made to facilitate and encourage foundation models that are diverse in their capabilities, and allow the researchers to focus on what matters – that is, designing and training their foundation model – rather than spending the majority of their time building and optimizing evaluation boilerplate. Furthermore, the diversity of signal that GATE provides allows better understanding of a given model’s strengths and weaknesses, which as a result makes the research, review and iteration process of the field as a whole more efficient. This is because there is a consistent boilerplate that runs all models, with broad signal that reduces probability of making erroneous conclusions – both in the overly optimistic, or overly pessimistic side of things.

6.1 Principal Use Cases

1. **Model Development and Iteration**: GATE serves as a valuable tool during the model research and development phase. By integrating the model into GATE and running either the smallGATE or baseGATE tiers, developers can obtain a comprehensive and robust performance evaluation of their model across diverse domains, tasks, and modalities. Worth noting that GATE allows easy inclusion of foundation models pretrained on images, video, audio, text, etc, to be fine-tuned on pixel-based tasks. It achieves this by replacing a model’s root layer / embedding layer, with one appropriate for a given task’s modality, and adding on top a relevant task adapter head.

2. **Model Evaluation for Machine Learning Research**: GATE enhances the communication of research findings and their potential applications, a vital aspect of scientific collaboration. By using GATE as a benchmark, even at the most cost-efficient GPU hour level of smallGATE, the clarity and depth of future ML papers can be significantly improved. GATE’s explicit evaluation of modality, domain, and task shifts in a given foundation model provides a nuanced and informative perspective on a model’s true capabilities, offering a more detailed understanding of a model’s strengths and weaknesses than optimizing a single metric, such as ImageNet validation error. 7 Result Extras

The results were logged in WandB, and then further processed after all experiments were completed to generate the tables and figures in this paper. Much of the logged information outside of testing metrics were not used for any of the figures and tables in this paper. The full set of experiments and all the logged results can be found at our wandb gate project repo.

7.1 Result Processing

Once all experiments were completed, we queried our wandb project repository and returned test results from all our experiments, if an experiment name was duplicated, we used the latest entries, and, for each experiment type there existed three independent runs. We averaged the results of any metrics across such independent runs to acquire a better approximation to the true performance of those models.

8 Preliminary Experiments Details

8.1 Preliminary Experiments

First, we trained models on ImageNet1k, CIFAR100, CLEVR, ADE20K, CityScapes, and, ACDC for 5K iterations, using cosine annealing learning schedule or plateau annealing, with AdamW, weight decays varying from 0.1 - 0.0001, and applied models from each major architecture category – specifically, the CLIPViT, ImageNet pretrained ViT, ResNext, ResNet and ConvNextV2. The results from these experiments pointed to the fact that there exists one general and good recipe for each architecture style. The recipes that we discovered were as follows:

8.1.1 Across Architecture Settings

Unless otherwise stated, the settings here are applied universally in all experiments.

**Optimizer**: AdamW, weight decay 0.01, plateau annealing with patience 1000, relative scaling and scale factor 0.5, and, threshold 0.0001.

**Training Details**: Training iterations: 10K, validate every 500 iterations.

**Test Details**: Top-3 validation models (across all validated checkpoints) are ensembled by prediction averaging.

8.1.2 Architecture Specific Settings

**Convolutional Architectures**: **Optimizer**: AdamW, learning rate 1e-3, and for segmentation tasks only, we used learning rate 6e-4

**Vision Transformer Architectures**: **Optimizer**: AdamW, learning rate 1e-5

**Convolutional + Transformer Hybrid Architectures**: **Optimizer**: AdamW, learning rate 2e-5

The above recipes were what we used throughout all our experiments unless otherwise stated.

9 GATE Guiding Principles

The fundamental values driving the design decisions behind GATE are the following:

1. Maximizing Generalization Signal: GATE is designed to provide a high signal-to-noise ratio concerning a model’s ability to generalize in diverse downstream contexts, that vary in domain, task and modality. This allows for a more robust assessment of a model’s capacity for adaptation and versatility. By noise here we refer to how clear a given signal response is. For example, an image classification test accuracy signal on ImageNet, would provide clear

---

2 omitted until double blind is over signal with respect to the natural domain and the classification task, but would be blurry for
more compositional, object disentanglement and relational tasks, such as segmentation, or,
visual question answering.

2. Time Efficiency: Acknowledging the importance of computational resources and time,
GATE operates within set benchmarks of 12, 24, and 36 GPU hours (established on A100 @
40GB). These set timeframes ensure GATE’s assessments are both thorough and expedient.

3. Minimizing Usage Friction: The framework supporting GATE is designed to be user-friendly,
enabling easy integration of new backbones and facilitating smooth experimentation. This
low-friction approach ensures a streamlined experience when using GATE, making the
process of evaluation more efficient.

We argue that a good balance of the above can generate a pragmatic, yet thorough foundation model
evaluation suite, that will, importantly, be of real use to most researchers in the field.

10 Defining the GATE Benchmark

GATE is a comprehensive evaluation engine designed to advance the development of more general
machine learning models. It improves on existing benchmarks by enabling the evaluation of models
across diverse modalities, domains, and tasks.

GATE is composed of three key components. The first is a benchmark pool, a broad collection of
datasets, tasks, and processes that measure a model’s performance across various domains, tasks,
and modalities. The second component is a set of benchmark tiers, which are meticulously curated
subsets from the GATE benchmark pool, tailored to specific compute budgets and project phases.
The final, and is a software framework, designed to seamlessly integrate new foundation models and
execute the GATE tiers, thereby enabling efficient performance evaluation across a diverse range of
downstream modalities, domains, and tasks. Practically, GATE is directed towards machine learning
researchers and developers as a means to efficiently, and with little friction, get broad signal about
how their model performs after transfer in diverse contexts, specifically selected for their empirically
evaluated high signal-to-noise ratio with respect to predictive power in how a model performs in
previously unseen contexts.

Building GATE was a careful balancing act. We needed to respect specific time budgets while also
aiming for a wide variety of evaluation scenarios. Our approach was as follows:

1. Select a diverse set of learning contexts, spanning multiple domains, tasks and modalities.
   We refer this as the Benchmark Pool.

2. Select a broad set of key foundation models, varying in their architecture, pretraining scheme
   and source modality. We refer to this as the Model Pool.

3. Fine tune each of the models in the model pool, on each of the contexts in the benchmark
   pool. Evaluate trained models on each context’s test sets.

4. Use the test set results acquired to quantify the predictive power each benchmark holds with
   respect to previously unseen benchmarks, both at the individual level and the collection
   level. We call this measure, the downstream generalization predictability measure (DGPM).

5. Use the DGPM values of the various combinations of benchmarks to build the three GATE
tiers, selecting combinations of benchmarks that can provide the most information within a
target time budget.

We elaborate on each of the above steps in the following subsections.

11 Benchmark Pool Selection Details

Medical Image Classification: Medical data are known to present a substantial shift in both domain
and even modality depending on their format. We have selected datasets that not only pose significant
challenges for foundation models but also align with the broader imperative to deliver real-world
benefits downstream. **Chexpert**: A dataset comprising a challenging array of chest x-rays annotated with findings critical to diagnosing thoracic diseases. It tests models on their ability to navigate complex, multi-label medical data, encapsulating the kind of nuanced decision-making that AI must augment in clinical settings.

**Diabetic Retinopathy Classification**: Early detection of diabetic retinopathy from retinal images is a public health priority; models fine-tuned on this dataset can have immediate implications for preventing vision loss on a global scale. This dataset requires models to decipher fine-grained, progressive changes indicative of the disease, reflecting the precision necessary for medical AI applications.

**HAM10000 (Human Against Machine with 10000 dermatoscopic images)**: The dataset provides a diverse spectrum of skin lesion images vital for differentiating between benign and malignant conditions. Incorporating this dataset not only challenges the pattern recognition prowess of AI but also contributes to the advancement of dermatology through machine learning technologies.

**Metrics**: We collect Average Precision Score (APS), Area Under the Receiver Operating Characteristics Curve (AUC), and Brier Score (BS) both overall (i.e. macro) as well as for individual pathologies/classes.

**Medical Segmentation**: This category evaluates foundational models’ ability to generalize from natural to medical image modalities and to perform domain-specific tasks that require precision and complex spatial understanding:

**ACDC (Automated Cardiac Diagnosis Challenge)**: This dataset is aimed at assessing models’ generalization to the medical domain, particularly the transferability of representations for segmenting anatomical structures in cardiac MRI images. By focusing on the heart’s intricate anatomy, ACDC tests the models’ ability to adapt to clinically relevant shapes and patterns—a shift from common visual recognition tasks to precise medical delineation. **Metrics**: We collect dice loss, mIoU, mean accuracy and overall accuracy.

### 12 Benchmark Pool Details

Having a set of diverse benchmarks ranging in challenge factor, as well as modality, task and domain shift was key. We explain in more detail why why consider these factors important in Appendix in more detail. We refer to this as our *benchmark pool*, and it consists of the following:

**Image Classification**: We employ ImageNet1k [9], CIFAR100 [28], Places365 [74], and Food101 [36] to cover diverse natural image domains. Additionally, we include HappyWhale [17] for a more challenging domain shift, aiding in wildlife research and providing an interesting test case for model evaluation.

**Few Shot Image Classification**: We use the MetaDataset task recipe on the Aircraft [37], Fungi [50], MiniImageNet [62], CUB200 [63], and Describable Features [69] datasets to evaluate task and domain shift robustness for an evaluation model.

**Zero Shot Text-Image Classification**: Another key setting is that of zero-shot text-image classification, on which many current key models were trained and evaluated [43]. We utilize Flickr30K, New Yorker Caption Context (a challenging humor task), and Winoground—a task requiring the model to match two texts with their corresponding images, focusing on compositional differences.

**Visual Relational Reasoning**: A context where earlier models, such as ResNet50 [18] had low performance without layers with associative inductive biases (e.g., relational neural networks or transformers [49, 61]). This ensures we are aware of any trade-offs in relational compositional abilities in our models. We use CLEVR [23] and CLEVRMath [34].

**Image Semantic Segmentation**: Essential for various real-world applications, serving as an indicator of a model’s ability to retain spatial information and identify objects at a per-pixel level. ADE20K [75], COCO10K [33], COCO164K [33], NYU-Depth-v2 [54], PascalContext [38], and Cityscapes [8].

**Medical Image Classification**: Medical data exhibit substantial domain and modality shifts, posing significant challenges for machine learning models while aligning with the imperative to deliver real-world benefits. **Chexpert** [21] (chest X-rays annotated for thoracic disease diagnosis), **Dia- **Betic Retinopathy Classification** [16] (retinal images for early detection of diabetic retinopathy), **HAM10000** [60] (dermatoscopic images for differentiating skin lesions).

**Medical Segmentation → ACDC (Automated Cardiac Diagnosis Challenge)** [5]: This dataset assesses models’ generalization to the medical domain, particularly the transferability of representations for segmenting anatomical structures in cardiac MRI images. By focusing on the heart’s intricate anatomy, ACDC tests the models’ ability to adapt to clinically relevant shapes and patterns.

**Video Classification**: Video classification tasks test models on their temporal generalization abilities and require an understanding of not only individual frame content but also the transition and context between frames. **HMDB51 (Human Motion Database)** [30], **UCF-101 (University of Central Florida - 101 action categories)** [56], **Kinetics400** [24].

**Video Regression**: Where classification tasks gauge categorical distinctions, video regression tasks assess models’ ability to make continuous numerical predictions from temporal data, serving as an indicator of a model’s capability to process and quantify dynamic content. **iWildcam (International Wildlife Camera Trap Challenge)** [4]: This dataset targets estimating animal species abundance from videos and is a direct test of modality and task shift, and showcases a models’ potential impact on ecological monitoring and species conservation efforts.

1. **Modality shifting** contexts: Contexts where the foundation model is asked to learn to do well at a task that requires understanding of a previously unseen modality. More specifically, assuming a foundation model has been trained on natural images, this would be transferring to medical imaging, video, audio and test contexts. This would shed light on the performance of a model’s middle layers.

2. **Task shifting** contexts: Contexts where a model is tasked with performing a previously unseen task, for example, transferring from classification to segmentation or relational reasoning.

3. **Domain shifting** contexts: Contexts where a model is required to perform a task on a domain that is different from the one it was trained on. For example moving from natural images on ImageNet at 224x224 resolution to black and white Omniglot characters at 28x28 resolution, or, moving from ImageNet to images of fungi. More extreme domain shifts would be going from natural images to medical images for example.

13 Model Pool Details

14 Task Adapter Details

15 Experimental Details

**Experimental Environment Details**: GPUs: 4 x A6000 Ada @ 48GB, CPUs: 128 Core AMD EPYC 7713 64-Core Processor, RAM: 1 TB, HD: 15TB NVME. All experiments were done with BF16 precision.

16 Additional Results

16.1 Full details on discovered combinations Figure 6: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different best combinations in varying $k$.  | Metric â | Model â | cvnxtv2 | siglip | clip | flex | deit | laion | vit | dino | smvit | rnx50 | effv2 | r50a1 | effrnr | seffv2 | sflex | svit | whspr | sr50a1 | bert | bart | mpnet |\n|----------|---------|--------|------|-----|-----|------|-----|-----|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\n| **Img Class** | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | Dataset          | Metric          | Value 1 | Value 2 | Value 3 | Value 4 | Value 5 | Value 6 | Value 7 | Value 8 | Value 9 | Value 10 | Value 11 | Value 12 | Value 13 | Value 14 | Value 15 | Value 16 | Value 17 | Value 18 | Value 19 | Value 20 | Value 21 | Value 22 | Value 23 | Value 24 | Value 25 | Value 26 | Value 27 | Value 28 | Value 29 | Value 30 | Value 31 | Value 32 | Value 33 | Value 34 | Value 35 | Value 36 | Value 37 | Value 38 | Value 39 | Value 40 | Value 41 | Value 42 | Value 43 | Value 44 | Value 45 | Value 46 | Value 47 | Value 48 | Value 49 | Value 50 | Value 51 | Value 52 | Value 53 | Value 54 | Value 55 | Value 56 | Value 57 | Value 58 | Value 59 | Value 60 | Value 61 | Value 62 | Value 63 | Value 64 | Value 65 | Value 66 | Value 67 | Value 68 | Value 69 | Value 70 | Value 71 | Value 72 | Value 73 | Value 74 | Value 75 | Value 76 | Value 77 | Value 78 | Value 79 | Value 80 | Value 81 | Value 82 | Value 83 | Value 84 | Value 85 | Value 86 | Value 87 | Value 88 | Value 89 | Value 90 | Value 91 | Value 92 | Value 93 | Value 94 | Value 95 | Value 96 | Value 97 | Value 98 | Value 99 | Value 100 | Value 101 | Value 102 | Value 103 | Value 104 | Value 105 | Value 106 | Value 107 | Value 108 | Value 109 | Value 110 | Value 111 | Value 112 | Value 113 | Value 114 | Value 115 | Value 116 | Value 117 | Value 118 | Value 119 | Value 120 | Value 121 | Value 122 | Value 123 | Value 124 | Value 125 | Value 126 | Value 127 | Value 128 | Value 129 | Value 130 | Value 131 | Value 132 | Value 133 | Value 134 | Value 135 | Value 136 | Value 137 | Value 138 | Value 139 | Value 140 | Value 141 | Value 142 | Value 143 | Value 144 | Value 145 | Value 146 | Value 147 | Value 148 | Value 149 | Value 150 | Value 151 | Value 152 | Value 153 | Value 154 | Value 155 | Value 156 | Value 157 | Value 158 | Value 159 | Value 160 | Value 161 | Value 162 | Value 163 | Value 164 | Value 165 | Value 166 | Value 167 | Value 168 | Value 169 | Value 170 | Value 171 | Value 172 | Value 173 | Value 174 | Value 175 | Value 176 | Value 177 | Value 178 | Value 179 | Value 180 | Value 181 | Value 182 | Value 183 | Value 184 | Value 185 | Value 186 | Value 187 | Value 188 | Value 189 | Value 190 | Value 191 | Value 192 | Value 193 | Value 194 | Value 195 | Value 196 | Value 197 | Value 198 | Value 199 | Value 200 | Value 201 | Value 202 | Value 203 | Value 204 | Value 205 | Value 206 | Value 207 | Value 208 | Value 209 | Value 210 | Value 211 | Value 212 | Value 213 | Value 214 | Value 215 | Value 216 | Value 217 | Value 218 | Value 219 | Value 220 | Value 221 | Value 222 | Value 223 | Value 224 | Value 225 | Value 226 | Value 227 | Value 228 | Value 229 | Value 230 | Value 231 | Value 232 | Value 233 | Value 234 | Value 235 | Value 236 | Value 237 | Value 238 | Value 239 | Value 240 | Value 241 | Value 242 | Value 243 | Value 244 | Value 245 | Value 246 | Value 247 | Value 248 | Value 249 | Value 250 | Value 251 | Value 252 | Value 253 | Value 254 | Value 255 | Value 256 | Value 257 | Value 258 | Value 259 | Value 260 | Value 261 | Value 262 | Value 263 | Value 264 | Value 265 | Value 266 | Value 267 | Value 268 | Value 269 | Value 270 | Value 271 | Value 272 | Value 273 | Value 274 | Value 275 | Value 276 | Value 277 | Value 278 | Value 279 | Value 280 | Value 281 | Value 282 | Value 283 | Value 284 | Value 285 | Value 286 | Value 287 | Value 288 | Value 289 | Value 290 | Value 291 | Value 292 | Value 293 | Value 294 | Value 295 | Value 296 | Value 297 | Value 298 | Value 299 | Value 300 | Value 301 | Value 302 | Value 303 | Value 304 | Value 305 | Value 306 | Value 307 | Value 308 | Value 309 | Value 310 | Value 311 | Value 312 | Value 313 | Value 314 | Value 315 | Value 316 | Value 317 | Value 318 | Value 319 | Value 320 | Value 321 | Value 322 | Value 323 | Value 324 | Value 325 | Value 326 | Value 327 | Value 328 | Value 329 | Value 330 | Value 331 | Value 332 | Value 333 | Value 334 | Value 335 | Value 336 | Value 337 | Value 338 | Value 339 | Value 340 | Value 341 | Value 342 | Value 343 | Value 344 | Value 345 | Value 346 | Value 347 | Value 348 | Value 349 | Value 350 | Value 351 | Value 352 | Value 353 | Value 354 | Value 355 | Value 356 | Value 357 | Value 358 | Value 359 | Value 360 | Value 361 | Value 362 | Value 363 | Value 364 | Value 365 | Value 366 | Value 367 | Value 368 | Value 369 | Value 370 | Value 371 | Value 372 | Value 373 | Value 374 | Value 375 | Value 376 | Value 377 | Value 378 | Value 379 | Value 380 | Value 381 | Value 382 | Value 383 | Value 384 | Value 385 | Value 386 | Value 387 | Value 388 | Value 389 | Value 390 | Value 391 | Value 392 | Value 393 | Value 394 | Value 395 | Value 396 | Value 397 | Value 398 | Value 399 | Value 400 | Value 401 | Value 402 | Value 403 | Value 404 | Value 405 | Value 406 | Value 407 | Value 408 | Value 409 | Value 410 | Value 411 | Value 412 | Value 413 | Value 414 | Value 415 | Value 416 | Value 417 | Value 418 | Value 419 | Value 420 | Value 421 | Value 422 | Value 423 | Value 424 | Value 425 | Value 426 | Value 427 | Value 428 | Value 429 | Value 430 | Value 431 | Value 432 | Value 433 | Value 434 | Value 435 | Value 436 | Value 437 | Value 438 | Value 439 | Value 440 | Value 441 | Value 442 | Value 443 | Value 444 | Value 445 | Value 446 | Value 447 | Value 448 | Value 449 | Value 450 | Value 451 | Value 452 | Value 453 | Value 454 | Value 455 | Value 456 | Value 457 | Value 458 | Value 459 | Value 460 | Value 461 | Value 462 | Value 463 | Value 464 | Value 465 | Value 466 | Value 467 | Value 468 | Value 469 | Value 470 | Value 471 | Value 472 | Value 473 | Value 474 | Value 475 | Value 476 | Value 477 | Value 478 | Value 479 | Value 480 | Value 481 | Value 482 | Value 483 | Value 484 | Value 485 | Value 486 | Value 487 | Value 488 | Value 489 | Value 490 | Value 491 | Value 492 | Value 493 | Value 494 | Value 495 | Value 496 | Value 497 | Value 498 | Value 499 | Value 500 | Value 501 | Value 502 | Value 503 | Value 504 | Value 505 | Value 506 | Value 507 | Value 508 | Value 509 | Value 510 | Value 511 | Value 512 | Value 513 | Value 514 | Value 515 | Value 516 | Value 517 | Value 518 | Value 519 | Value 520 | Value 521 | Value 522 | Value 523 | Value 524 | Value 525 | Value 526 | Value 527 | Value 528 | Value 529 | Value 530 | Value 531 | Value 532 | Value 533 | Value 534 | Value 535 | Value 536 | Value 537 | Value 538 | Value 539 | Value 540 | Value 541 | Value 542 | Value 543 | Value 544 | Value 545 | Value 546 | Value 547 | Value 548 | Value 549 | Value 550 | Value 551 | Value 552 | Value 553 | Value 554 | Value 555 | Value 556 | Value 557 | Value 558 | Value 559 | Value 560 | Value 561 | Value 562 | Value 563 | Value 564 | Value 565 | Value 566 | Value 567 | Value 568 | Value 569 | Value 570 | Value 571 | Value 572 | Value 573 | Value 574 | Value 575 | Value 576 | Value 577 | Value 578 | Value 579 | Value 580 | Value 581 | Value 582 | Value 583 | Value 584 | Value 585 | Value 586 | Value 587 | Value 588 | Value 589 | Value 590 | Value 591 | Value 592 | Value 593 | Value 594 | Value 595 | Value 596 | Value 597 | Value 598 | Value 599 | Value 600 | Value 601 | Value 602 | Value 603 | Value 604 | Value 605 | Value 606 | Value 607 | Value 608 | Value 609 | Value 610 | Value 611 | Value 612 | Value 613 | Value 614 | Value 615 | Value 616 | Value 617 | Value 618 | Value 619 | Value 620 | Value 621 | Value 622 | Value 623 | Value 624 | Value 625 | Value 626 | Value 627 | Value 628 | Value 629 | Value 630 | Value 631 | Value 632 | Value 633 | Value 634 | Value 635 | Value 636 | Value 637 | Value 638 | Value 639 | Value 640 | Value 641 | Value 642 | Value 643 | Value 644 | Value 645 | Value 646 | Value 647 | Value 648 | Value 649 | Value 650 | Value 651 | Value 652 | Value 653 | Value 654 | Value 655 | Value 656 | Value 657 | Value 658 | Value 659 | Value 660 | Value 661 | Value 662 | Value 663 | Value 664 | Value 665 | Value 666 | Value 667 | Value 668 | Value 669 | Value 670 | Value 671 | Value 672 | Value 673 | Value 674 | Value 675 | Value 676 | Value 677 | Value 678 | Value 679 | Value 680 | Value 681 | Value 682 | Value 683 | Value 684 | Value 685 | Value 686 | Value 687 | Value 688 | Value 689 | Value 690 | Value 691 | Value 692 | Value 693 | Value 694 | Value  |                      | Kinetics Acc@1 | Kinetics Acc@5 | Kinetics Loss | UCF-101 Acc@1 | UCF-101 Acc@5 | UCF-101 Loss | Task Mean | IWildCam MAE Score | IWildCam MSE Loss | Task Mean |
|----------------------|----------------|----------------|---------------|---------------|---------------|--------------|-----------|-------------------|------------------|-----------|
|                      | 48.8           | 44.2           | 43.7          | 40.3          | 44.6          | 33.2         | 36.4       | 25.8              | 2.7              | 1.0       |
|                      | 75.5           | 70.9           | 77.9          | 70.7          | 67.6          | 71.7         | 59.9       | 63.0              | 9.7              | 4.3       |
|                      | 2.4            | 2.6            | 2.5           | 2.7           | 2.5           | 3.2          | 3.0        | 3.5               | 5.5              | 6.1       |
|                      | 84.4           | 75.1           | 69.9          | 63.2          | 75.0          | 63.4         | 58.8       | 66.6              | 48.7             | 19.7      |
|                      | 95.4           | 92.5           | 89.1          | 82.3          | 91.6          | 86.2         | 81.7       | 86.3              | 75.3             | 42.2      |
|                      | 0.6            | 1.0            | 1.3           | 1.7           | 1.0           | 1.5          | 1.7        | 1.4               | 2.3              | 4.3       |
|                      | 73.0           | 65.6           | 66.6          | 58.8          | 63.7          | 57.5         | 53.3       | 57.5              | 49.8             | 17.2      |
|                      | 1.3            | 1.4            | 1.3           | 1.4           | 1.4           | 1.6          | 1.4        | 1.5               | 1.6              | 2.0       |
|                      | 3.7            | 4.4            | 4.0           | 4.0           | 4.1           | 5.4          | 4.3        | 5.0               | 5.9              | 7.1       |
|                      | 1.3            | 1.4            | 1.3           | 1.4           | 1.4           | 1.6          | 1.4        | 1.5               | 1.6              | 2.0       |
|                      | 69.0           | 66.8           | 66.8          | 64.6          | 64.3          | 63.4         | 62.1       | 62.2              | 58.5             | 56.3      |
|                      | 76.6           | 74.5           | 74.4          | 72.8          | 72.0          | 71.9         | 70.6       | 70.0              | 66.8             | 66.7      |
|                      | 68.3           | 65.6           | 65.7          | 62.6          | 63.7          | 60.7         | 60.2       | 60.7              | 58.6             | 55.1      |
|                      | 77.7           | 74.9           | 74.6          | 73.3          | 72.4          | 71.2         | 68.9       | 69.1              | 65.3             | 65.7      |
|                      | 1.0            | 3.0            | 2.0           | 4.0           | 5.0           | 6.0          | 8.0        | 7.0               | 9.0              | 10.0      |
|                      | 1.0            | 2.0            | 3.0           | 4.0           | 5.0           | 6.0          | 7.0        | 8.0               | 9.0              | 10.0      |
|                      | 1.0            | 3.0            | 2.0           | 5.0           | 4.0           | 7.0          | 8.0        | 6.0               | 9.0              | 10.0      |
|                      | 1.0            | 2.0            | 3.0           | 4.0           | 5.0           | 6.0          | 8.0        | 7.0               | 10.0             | 9.0       |

Table 3: Full experiments table: Black/Bold best model, Green second best, Blue third best, and red the worst performing model. Models prefixed with ’s’ refer to ’from scratch’ trained models, rather than pretrained. This table showcases the full set of data we use to evolve GATE using EEVEE. Figure 7: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different best combinations in varying $k$.  (a) Best k=24 discovered combination

(b) Best k=25 discovered combination

(c) Best k=26 discovered combination

(d) Best k=27 discovered combination

Figure 8: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different best combinations in varying $k$.

Figure 9: Ranking Heatmap for bigGATE We show how the various models on the y-axis rank on the metrics on the x-axis, where brighter is higher/better rank. From left to right we apply a spearman correlation sorting to capture tasks more similar to imagenet1k more towards the leftmost side, and, dissimilar ones towards the rightmost side. From top to bottom we rank models based on average rank. Figure 10: Ranking Heatmap for baseGATE: We show how the various models on the y-axis rank on the metrics on the x-axis, where brighter is higher/better rank. From left to right we apply a spearman correlation sorting to capture tasks more similar to imagenet1k more towards the leftmost side, and, dissimilar ones towards the rightmost side. From top to bottom we rank models based on average rank. Figure 11: Ranking Heatmap for smallGATE: We show how the various models on the y-axis rank on the metrics on the x-axis, where brighter is higher/better rank. From left to right we apply a spearman correlation sorting to capture tasks more similar to imagenet1k more towards the leftmost side, and, dissimilar ones towards the rightmost side. From top to bottom we rank models based on average rank. Figure 12: Architecture Variation: Results of keeping the pretraining method the same as ImageNet1k classification and varying the architecture across various key task domains.

Figure 13: Pretraining Scheme Variation: Results of varying the pretraining method and keeping the architecture as ViT B16 across various key task domains. Figure 14: Modality Variation: Results of attempting modality shifting from audio and text to vision tasks.

Figure 15: Modality Variation: Results of attempting modality shifting from audio and text to vision tasks. 1. **Claims**

   Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?

   Answer: [Yes]

   Justification: All the claims made are substantiated with rigorous empirical results and communicated via tables and figures.

   Guidelines:

   - The answer NA means that the abstract and introduction do not include the claims made in the paper.
   - The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
   - The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
   - It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

2. **Limitations**

   Question: Does the paper discuss the limitations of the work performed by the authors?

   Answer: [Yes]

   Justification: We have an explicit limitations section.

   Guidelines:

   - The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
   - The authors are encouraged to create a separate "Limitations" section in their paper.
   - The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
   - The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
   - The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
   - The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
   - If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
   - While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs**

   Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

   Answer: [NA] Justification: No theories were derived.

Guidelines:

- The answer NA means that the paper does not include theoretical results.
- All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
- All assumptions should be clearly stated or referenced in the statement of any theorems.
- The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
- Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
- Theorems and Lemmas that the proof relies upon should be properly referenced.

4. **Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes]

Justification: We do so both in the main paper, and in more detail in the appendix, in addition to offering the codebase that reproduces all results.

Guidelines:

- The answer NA means that the paper does not include experiments.
- If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
- If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
- Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
- While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code**

Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]

Justification: Full code and data are available and shared on github and huggingface.

Guidelines:

- The answer NA means that paper does not include experiments requiring code.
- Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
- While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
- The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
- The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
- The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
- At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
- Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details**

   Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

   Answer: [Yes]

   Justification: We describe these in the experiments section in summary, and in the appendix in detail.

   Guidelines:

   - The answer NA means that the paper does not include experiments.
   - The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
   - The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance**

   Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

   Answer: [Yes]

   Justification: Where relevant our results include error bars.

   Guidelines:

   - The answer NA means that the paper does not include experiments.
   - The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
   - The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
   - The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
   - The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. Experiments Compute Resources

Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

Answer: [TODO]

Justification: [TODO]

Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).

9. Code Of Ethics

Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines]? 

Answer: [Yes]

Justification: Yes it does abide by the code of ethics to our best of our understanding.

Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. Broader Impacts

Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

Answer: [No]

Justification: It’s a method for finding optimal subsets of benchmarks from a large pool and a framework that automates model encoder evaluation. Societal impacts relate to improved research efficiency and hopefully compute usage, however this is too far from what one would consider strongly tied societal impacts.

Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. Safeguards

Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

Answer: [NA]

Justification: It’s a benchmark with datasets that are already public and previously published in other papers.

Guidelines:

• The answer NA means that the paper poses no such risks.

• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

• Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. Licenses for existing assets

Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

Answer: [Yes]

Justification: All datasets used have appropriate licenses, and the code packages used in implementing our software framework have appropriate licenses as well.

Guidelines:

• The answer NA means that the paper does not use existing assets.

• The authors should cite the original paper that produced the code package or dataset.

• The authors should state which version of the asset is used and, if possible, include a URL.

• The name of the license (e.g., CC-BY 4.0) should be included for each asset.

• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

• If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the asset’s creators.

13. **New Assets**

   Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

   Answer: [Yes]

   Justification: Our codebase is fully documented.

   Guidelines:
   • The answer NA means that the paper does not release new assets.
   • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
   • The paper should discuss whether and how consent was obtained from people whose asset is used.
   • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects**

   Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

   Answer: [NA]

   Justification: No crowdsourcing with humans

   Guidelines:
   • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
   • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
   • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

   Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

   Answer: [NA]

   Justification: Same as previous answer.

   Guidelines:
   • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
   • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
   • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
   • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.